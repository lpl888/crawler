{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "class AdCrawler:\n",
    "    def __init__(self, base_url, max_pages=2, retry_limit=3):\n",
    "        self.base_url = base_url\n",
    "        self.driver = None\n",
    "        self.all_ads = []\n",
    "        self.max_pages = max_pages\n",
    "        self.retry_limit = retry_limit  # Stale element 발생 시 최대 재시도 횟수\n",
    "\n",
    "    def start_driver(self):\n",
    "        \"\"\"브라우저 드라이버를 시작\"\"\"\n",
    "        self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "    def stop_driver(self):\n",
    "        \"\"\"브라우저 드라이버를 종료\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "    def load_page(self, page_number):\n",
    "        \"\"\"페이지 번호에 맞는 URL을 로드\"\"\"\n",
    "        url = f\"{self.base_url}?page={page_number}\"\n",
    "        self.driver.get(url)\n",
    "        try:\n",
    "            # 페이지가 완전히 로드될 때까지 대기\n",
    "            WebDriverWait(self.driver, 150).until(\n",
    "                EC.presence_of_element_located((By.TAG_NAME, 'article'))  # article 태그 내의 요소가 나타날 때까지 대기\n",
    "            )\n",
    "            print(f\"--- Page {page_number} loaded ---\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading page {page_number}: {e}\")\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Page {page_number} does not exist or failed to load: {e}\")\n",
    "        #     return False\n",
    "\n",
    "    def extract_ad_data(self, ad_xpath, retry_count=0):\n",
    "        \"\"\"단일 광고의 데이터를 추출\"\"\"\n",
    "        try:\n",
    "            ad = self.driver.find_element(By.XPATH, ad_xpath)  # StaleElement 발생 시 요소를 다시 찾음\n",
    "            # 재귀적으로 모든 자식 요소에서 텍스트를 추출하는 함수\n",
    "            def extract_text_from_element(element):\n",
    "                text = element.text.strip() if element.text.strip() else \"\"\n",
    "                children_text = []\n",
    "                # 자식 요소들에서 텍스트 추출\n",
    "                for child in element.find_elements(By.XPATH, './*'):\n",
    "                    children_text.extend(extract_text_from_element(child))\n",
    "                return [text] + children_text if text else children_text\n",
    "\n",
    "            # article 내 모든 텍스트를 리스트로 추출\n",
    "            all_texts = extract_text_from_element(ad)[0].split('\\n')\n",
    "\n",
    "            if len(all_texts) >= 5:\n",
    "                title = all_texts[0]  # 첫 번째 텍스트는 제목\n",
    "                link = ad.find_element(By.TAG_NAME, 'a').get_attribute('href')  # 링크는 직접 추출\n",
    "                cost = all_texts[3].replace('~','')  # 가격\n",
    "                rating = all_texts[1]  # 평점\n",
    "                reviews = all_texts[2].replace('(', '').replace(')', '')  # 리뷰 수\n",
    "                company_name = all_texts[4]  # 기업명\n",
    "            \n",
    "            elif len(all_texts) >= 3:\n",
    "                # ['고객에게 각인되는 워드프레스 반응형 홈페이지 제작', '290,000원', '리플래닝']\n",
    "                title = all_texts[0]  # 첫 번째 텍스트는 제목\n",
    "                link = ad.find_element(By.TAG_NAME, 'a').get_attribute('href')  # 링크는 직접 추출\n",
    "                cost = all_texts[1].replace('~','')  # 가격\n",
    "                rating = 0\n",
    "                reviews = 0\n",
    "                company_name = all_texts[2]  # 기업명\n",
    "            \n",
    "\n",
    "            # 추출된 데이터를 사전에 저장\n",
    "            ad_data = {\n",
    "                'title': title,\n",
    "                'link': link,\n",
    "                'cost': cost,\n",
    "                'rating': rating,\n",
    "                'reviews': reviews,\n",
    "                'company_name': company_name,\n",
    "            }\n",
    "            # print(ad_data)\n",
    "            return ad_data\n",
    "\n",
    "        except StaleElementReferenceException:\n",
    "            if retry_count < self.retry_limit:\n",
    "                print(f\"Stale element detected, retrying... attempt {retry_count + 1}\")\n",
    "                time.sleep(1)  # 짧은 대기 후 재시도\n",
    "                return self.extract_ad_data(ad_xpath, retry_count + 1)  # 요소를 다시 찾고 재시도\n",
    "            else:\n",
    "                print(\"Stale element could not be recovered after multiple retries.\")\n",
    "                return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing ad: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_ads(self):\n",
    "        \"\"\"1페이지부터 max_pages 페이지까지 순차적으로 크롤링\"\"\"\n",
    "        self.start_driver()\n",
    "\n",
    "        try:\n",
    "            for page in range(1, self.max_pages + 1):\n",
    "                print(f\"--- Scraping page {page} ---\")\n",
    "                \n",
    "                # 해당 페이지 로드\n",
    "                self.load_page(page_number=page)\n",
    "                \n",
    "                try:\n",
    "                    # 페이지가 완전히 로드될 때까지 대기\n",
    "                    WebDriverWait(self.driver, 15).until(\n",
    "                        EC.presence_of_element_located((By.TAG_NAME, 'article'))\n",
    "                    )\n",
    "                    time.sleep(2)\n",
    "                    \n",
    "                except TimeoutException:\n",
    "                    print(f\"No articles found on page {page}. Ending scraping.\")\n",
    "                    break  # 더 이상 페이지를 로드하지 않고 루프 종료\n",
    "\n",
    "                \n",
    "\n",
    "                # 모든 article 태그를 리스트로 가져옴\n",
    "                ads = self.driver.find_elements(By.XPATH, '//article')\n",
    "\n",
    "                if not ads:\n",
    "                    print(f\"No articles found on page {page}.\")\n",
    "                    # continue\n",
    "                    break\n",
    "\n",
    "                for i, ad in enumerate(ads):\n",
    "                    ad_xpath = f'(//article)[{i+1}]'  # 각 광고의 XPATH를 추출\n",
    "                    ad_data = self.extract_ad_data(ad_xpath)  # XPATH를 기반으로 데이터 추출\n",
    "                    if ad_data:\n",
    "                        self.all_ads.append(ad_data)\n",
    "\n",
    "        finally:\n",
    "            self.stop_driver()\n",
    "\n",
    "    def save_json(self, file_name='ads_data.json'):\n",
    "        \"\"\"추출된 데이터를 JSON 파일로 저장\"\"\"\n",
    "        import json\n",
    "        with open(file_name, 'w', encoding='utf-8') as json_file:\n",
    "            json.dump(self.all_ads, json_file, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_excel('크몽_카테고리별_링크.xlsx')\n",
    "dataset = []\n",
    "for index, row in df.iterrows():\n",
    "    dataset.append({\n",
    "        'major' : row['대분류'],\n",
    "        'medium' : row['중분류'],\n",
    "        'sub' : row['소분류'],\n",
    "        'link' : row['링크'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_웹빌더_워드프레스_신규제작.json\n",
      "--- Scraping page 1 ---\n",
      "--- Page 1 loaded ---\n",
      "--- Scraping page 2 ---\n",
      "--- Page 2 loaded ---\n",
      "--- Scraping page 3 ---\n",
      "--- Page 3 loaded ---\n",
      "--- Scraping page 4 ---\n",
      "--- Page 4 loaded ---\n",
      "--- Scraping page 5 ---\n",
      "--- Page 5 loaded ---\n",
      "--- Scraping page 6 ---\n",
      "--- Page 6 loaded ---\n",
      "--- Scraping page 7 ---\n",
      "--- Page 7 loaded ---\n",
      "--- Scraping page 8 ---\n",
      "--- Page 8 loaded ---\n",
      "--- Scraping page 9 ---\n",
      "--- Page 9 loaded ---\n",
      "--- Scraping page 10 ---\n",
      "--- Page 10 loaded ---\n",
      "--- Scraping page 11 ---\n",
      "--- Page 11 loaded ---\n",
      "--- Scraping page 12 ---\n",
      "--- Page 12 loaded ---\n",
      "--- Scraping page 13 ---\n",
      "--- Page 13 loaded ---\n",
      "--- Scraping page 14 ---\n",
      "--- Page 14 loaded ---\n",
      "--- Scraping page 15 ---\n",
      "Error loading page 15: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00836AB3+25587]\n",
      "\t(No symbol) [0x007C9C54]\n",
      "\t(No symbol) [0x006C2113]\n",
      "\t(No symbol) [0x00706F62]\n",
      "\t(No symbol) [0x007071AB]\n",
      "\t(No symbol) [0x00747852]\n",
      "\t(No symbol) [0x0072ABE4]\n",
      "\t(No symbol) [0x00745370]\n",
      "\t(No symbol) [0x0072A936]\n",
      "\t(No symbol) [0x006FBA73]\n",
      "\t(No symbol) [0x006FC4CD]\n",
      "\tGetHandleVerifier [0x00B14C63+3032483]\n",
      "\tGetHandleVerifier [0x00B66B99+3368153]\n",
      "\tGetHandleVerifier [0x008C8F62+624802]\n",
      "\tGetHandleVerifier [0x008D07DC+655644]\n",
      "\t(No symbol) [0x007D260D]\n",
      "\t(No symbol) [0x007CF6D8]\n",
      "\t(No symbol) [0x007CF875]\n",
      "\t(No symbol) [0x007C1CA6]\n",
      "\tBaseThreadInitThunk [0x76EBFCC9+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77EA80CE+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77EA809E+238]\n",
      "\n",
      "No articles found on page 15. Ending scraping.\n",
      "1_웹빌더_워드프레스_유지보수.json\n",
      "--- Scraping page 1 ---\n",
      "--- Page 1 loaded ---\n",
      "--- Scraping page 2 ---\n",
      "--- Page 2 loaded ---\n",
      "--- Scraping page 3 ---\n",
      "--- Page 3 loaded ---\n",
      "--- Scraping page 4 ---\n",
      "--- Page 4 loaded ---\n",
      "No articles found on page 4.\n",
      "2_웹빌더_카페24_신규제작.json\n",
      "--- Scraping page 1 ---\n",
      "--- Page 1 loaded ---\n",
      "--- Scraping page 2 ---\n",
      "--- Page 2 loaded ---\n",
      "--- Scraping page 3 ---\n",
      "--- Page 3 loaded ---\n",
      "--- Scraping page 4 ---\n",
      "--- Page 4 loaded ---\n",
      "--- Scraping page 5 ---\n",
      "--- Page 5 loaded ---\n",
      "--- Scraping page 6 ---\n",
      "--- Page 6 loaded ---\n",
      "No articles found on page 6.\n",
      "3_웹빌더_카페24_유지보수.json\n",
      "--- Scraping page 1 ---\n",
      "--- Page 1 loaded ---\n",
      "--- Scraping page 2 ---\n",
      "--- Page 2 loaded ---\n",
      "--- Scraping page 3 ---\n",
      "--- Page 3 loaded ---\n",
      "--- Scraping page 4 ---\n",
      "--- Page 4 loaded ---\n",
      "--- Scraping page 5 ---\n",
      "--- Page 5 loaded ---\n",
      "--- Scraping page 6 ---\n",
      "--- Page 6 loaded ---\n",
      "No articles found on page 6.\n",
      "4_웹빌더_아임웹_신규제작.json\n",
      "--- Scraping page 1 ---\n",
      "--- Page 1 loaded ---\n",
      "--- Scraping page 2 ---\n",
      "--- Page 2 loaded ---\n",
      "--- Scraping page 3 ---\n",
      "--- Page 3 loaded ---\n",
      "--- Scraping page 4 ---\n",
      "--- Page 4 loaded ---\n",
      "--- Scraping page 5 ---\n",
      "--- Page 5 loaded ---\n",
      "--- Scraping page 6 ---\n",
      "--- Page 6 loaded ---\n",
      "--- Scraping page 7 ---\n",
      "--- Page 7 loaded ---\n",
      "Stale element detected, retrying... attempt 1\n",
      "--- Scraping page 8 ---\n",
      "--- Page 8 loaded ---\n",
      "No articles found on page 8.\n",
      "5_웹빌더_아임웹_유지보수.json\n",
      "--- Scraping page 1 ---\n",
      "--- Page 1 loaded ---\n",
      "--- Scraping page 2 ---\n",
      "--- Page 2 loaded ---\n",
      "No articles found on page 2.\n",
      "6_웹빌더_노션_신규제작.json\n",
      "--- Scraping page 1 ---\n",
      "--- Page 1 loaded ---\n",
      "--- Scraping page 2 ---\n",
      "--- Page 2 loaded ---\n",
      "No articles found on page 2.\n",
      "7_웹빌더_노션_유지보수.json\n",
      "--- Scraping page 1 ---\n",
      "--- Page 1 loaded ---\n",
      "--- Scraping page 2 ---\n",
      "--- Page 2 loaded ---\n",
      "No articles found on page 2.\n",
      "8_웹 제작_홈페이지 신규 제작_홈페이지 자체 개발.json\n",
      "--- Scraping page 1 ---\n",
      "--- Page 1 loaded ---\n",
      "--- Scraping page 2 ---\n",
      "--- Page 2 loaded ---\n",
      "--- Scraping page 3 ---\n",
      "--- Page 3 loaded ---\n",
      "--- Scraping page 4 ---\n",
      "--- Page 4 loaded ---\n",
      "--- Scraping page 5 ---\n",
      "--- Page 5 loaded ---\n",
      "--- Scraping page 6 ---\n",
      "--- Page 6 loaded ---\n",
      "--- Scraping page 7 ---\n",
      "--- Page 7 loaded ---\n",
      "--- Scraping page 8 ---\n",
      "--- Page 8 loaded ---\n",
      "--- Scraping page 9 ---\n",
      "--- Page 9 loaded ---\n",
      "Error processing ad: list index out of range\n",
      "Stale element detected, retrying... attempt 1\n",
      "--- Scraping page 10 ---\n",
      "--- Page 10 loaded ---\n",
      "--- Scraping page 11 ---\n",
      "--- Page 11 loaded ---\n",
      "--- Scraping page 12 ---\n",
      "--- Page 12 loaded ---\n",
      "--- Scraping page 13 ---\n",
      "--- Page 13 loaded ---\n",
      "--- Scraping page 14 ---\n",
      "--- Page 14 loaded ---\n",
      "--- Scraping page 15 ---\n",
      "--- Page 15 loaded ---\n",
      "--- Scraping page 16 ---\n",
      "--- Page 16 loaded ---\n",
      "--- Scraping page 17 ---\n",
      "--- Page 17 loaded ---\n",
      "--- Scraping page 18 ---\n",
      "--- Page 18 loaded ---\n",
      "--- Scraping page 19 ---\n",
      "--- Page 19 loaded ---\n",
      "--- Scraping page 20 ---\n",
      "--- Page 20 loaded ---\n",
      "--- Scraping page 21 ---\n",
      "--- Page 21 loaded ---\n",
      "--- Scraping page 22 ---\n",
      "--- Page 22 loaded ---\n",
      "--- Scraping page 23 ---\n",
      "--- Page 23 loaded ---\n",
      "--- Scraping page 24 ---\n",
      "--- Page 24 loaded ---\n",
      "--- Scraping page 25 ---\n",
      "--- Page 25 loaded ---\n",
      "--- Scraping page 26 ---\n",
      "--- Page 26 loaded ---\n",
      "--- Scraping page 27 ---\n",
      "--- Page 27 loaded ---\n",
      "--- Scraping page 28 ---\n",
      "--- Page 28 loaded ---\n",
      "--- Scraping page 29 ---\n",
      "--- Page 29 loaded ---\n",
      "--- Scraping page 30 ---\n",
      "--- Page 30 loaded ---\n",
      "--- Scraping page 31 ---\n",
      "--- Page 31 loaded ---\n",
      "--- Scraping page 32 ---\n",
      "--- Page 32 loaded ---\n",
      "No articles found on page 32.\n",
      "9_웹 제작_홈페이지 신규 제작_홈페이지 웹빌더.json\n",
      "--- Scraping page 1 ---\n",
      "--- Page 1 loaded ---\n",
      "--- Scraping page 2 ---\n",
      "--- Page 2 loaded ---\n",
      "--- Scraping page 3 ---\n",
      "--- Page 3 loaded ---\n",
      "--- Scraping page 4 ---\n",
      "--- Page 4 loaded ---\n",
      "--- Scraping page 5 ---\n",
      "--- Page 5 loaded ---\n",
      "--- Scraping page 6 ---\n",
      "--- Page 6 loaded ---\n",
      "--- Scraping page 7 ---\n",
      "--- Page 7 loaded ---\n",
      "--- Scraping page 8 ---\n",
      "--- Page 8 loaded ---\n",
      "--- Scraping page 9 ---\n",
      "--- Page 9 loaded ---\n",
      "--- Scraping page 10 ---\n",
      "--- Page 10 loaded ---\n",
      "--- Scraping page 11 ---\n",
      "--- Page 11 loaded ---\n",
      "--- Scraping page 12 ---\n",
      "--- Page 12 loaded ---\n",
      "--- Scraping page 13 ---\n",
      "--- Page 13 loaded ---\n",
      "No articles found on page 13.\n",
      "10_웹 제작_쇼핑몰 신규 제작_쇼핑몰 자체 개발.json\n",
      "--- Scraping page 1 ---\n",
      "--- Page 1 loaded ---\n",
      "--- Scraping page 2 ---\n",
      "--- Page 2 loaded ---\n",
      "--- Scraping page 3 ---\n",
      "--- Page 3 loaded ---\n",
      "--- Scraping page 4 ---\n",
      "--- Page 4 loaded ---\n",
      "No articles found on page 4.\n",
      "11_웹 제작_쇼핑몰 신규 제작_쇼핑몰 웹빌더.json\n",
      "--- Scraping page 1 ---\n",
      "--- Page 1 loaded ---\n",
      "--- Scraping page 2 ---\n",
      "--- Page 2 loaded ---\n",
      "--- Scraping page 3 ---\n",
      "--- Page 3 loaded ---\n",
      "--- Scraping page 4 ---\n",
      "Error loading page 4: Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00836AB3+25587]\n",
      "\t(No symbol) [0x007C9C54]\n",
      "\t(No symbol) [0x006C2113]\n",
      "\t(No symbol) [0x00706F62]\n",
      "\t(No symbol) [0x007071AB]\n",
      "\t(No symbol) [0x00747852]\n",
      "\t(No symbol) [0x0072ABE4]\n",
      "\t(No symbol) [0x00745370]\n",
      "\t(No symbol) [0x0072A936]\n",
      "\t(No symbol) [0x006FBA73]\n",
      "\t(No symbol) [0x006FC4CD]\n",
      "\tGetHandleVerifier [0x00B14C63+3032483]\n",
      "\tGetHandleVerifier [0x00B66B99+3368153]\n",
      "\tGetHandleVerifier [0x008C8F62+624802]\n",
      "\tGetHandleVerifier [0x008D07DC+655644]\n",
      "\t(No symbol) [0x007D260D]\n",
      "\t(No symbol) [0x007CF6D8]\n",
      "\t(No symbol) [0x007CF875]\n",
      "\t(No symbol) [0x007C1CA6]\n",
      "\tBaseThreadInitThunk [0x76EBFCC9+25]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77EA80CE+286]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x77EA809E+238]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(file_name)\n\u001b[0;32m      4\u001b[0m crawler \u001b[38;5;241m=\u001b[39m AdCrawler(contents[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m'\u001b[39m], max_pages\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mcrawler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape_ads\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m crawler\u001b[38;5;241m.\u001b[39msave_json(file_name\u001b[38;5;241m=\u001b[39mfile_name)\n",
      "Cell \u001b[1;32mIn[1], line 115\u001b[0m, in \u001b[0;36mAdCrawler.scrape_ads\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_page(page_number\u001b[38;5;241m=\u001b[39mpage)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m# 페이지가 완전히 로드될 때까지 대기\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m     \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTAG_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marticle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutException:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo articles found on page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Ending scraping.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\User\\anaconda3\\envs\\crawling\\Lib\\site-packages\\selenium\\webdriver\\support\\wait.py:102\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    100\u001b[0m     screen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscreen\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    101\u001b[0m     stacktrace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(exc, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstacktrace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 102\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m>\u001b[39m end_time:\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, contents in enumerate(dataset):\n",
    "    file_name = str(i) + '_' + contents['major'] + '_' + contents['medium'] + '_' + contents['sub'] + '.json'\n",
    "    print(file_name)\n",
    "    crawler = AdCrawler(contents['link'], max_pages=500)\n",
    "    crawler.scrape_ads()\n",
    "    crawler.save_json(file_name=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crawling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
